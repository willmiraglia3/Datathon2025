{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from catboost import CatBoostRegressor\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import optuna\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data and create train + test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_path = '/Users/teymour/Desktop/Datathon/data/data_dictionary.xlsx'\n",
    "data_dictionary = pd.read_excel(data_dict_path)\n",
    "\n",
    "scoring_path = '/Users/teymour/Desktop/Datathon/data/scoring.xlsx'\n",
    "scoring = pd.read_excel(scoring_path)\n",
    "\n",
    "submission_format_path = '/Users/teymour/Desktop/Datathon/data/submission_format.csv'\n",
    "submission_format = pd.read_csv(submission_format_path)\n",
    "\n",
    "training_path = '/Users/teymour/Desktop/Datathon/data/training.xlsx'\n",
    "training = pd.read_excel(training_path).dropna()\n",
    "\n",
    "# new data from Fred: https://fred.stlouisfed.org/series/CUSR0000SETA02\n",
    "cpi_used_cars_path = '/Users/teymour/Desktop/Datathon/data/CPI_UsedCars_US.xlsx'\n",
    "cpi_used_cars = pd.read_excel(cpi_used_cars_path, sheet_name='Monthly')\n",
    "cpi_used_cars.columns = ['Date', 'CPI']\n",
    "cpi_used_cars['Year'] = cpi_used_cars['Date'].dt.year\n",
    "average_cpi_by_year = cpi_used_cars.groupby('Year')['CPI'].mean().reset_index().rename(columns={'Year': 'Model Year'})\n",
    "\n",
    "def forecast_cpi_polynomial(average_cpi_by_year, forecast_periods=5, degree=2):\n",
    "    average_cpi_by_year = average_cpi_by_year.sort_values(by='Model Year')\n",
    "\n",
    "    X = average_cpi_by_year[['Model Year']]\n",
    "    y = average_cpi_by_year['CPI']\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "\n",
    "    last_year = X['Model Year'].max()\n",
    "    future_years = np.arange(last_year + 1, last_year + 1 + forecast_periods).reshape(-1, 1)\n",
    "\n",
    "    future_years_poly = poly.transform(future_years)\n",
    "\n",
    "    future_cpi = model.predict(future_years_poly)\n",
    "\n",
    "    future_df = pd.DataFrame({\n",
    "        'Model Year': future_years.flatten(),\n",
    "        'CPI': future_cpi\n",
    "    })\n",
    "\n",
    "    combined_df = pd.concat([average_cpi_by_year, future_df], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "average_cpi_by_year = forecast_cpi_polynomial(average_cpi_by_year, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input_data(data):\n",
    "    # add the CPI of Used Cars by City Average\n",
    "    clean_data = data.merge(average_cpi_by_year, on='Model Year', how='left')\n",
    "    clean_data['Model Year'] = clean_data['Model Year'].astype(str)\n",
    "    \n",
    "    # handle nulls\n",
    "    clean_data = clean_data.replace({\"nan\": np.nan})\n",
    "    clean_data['Model Year'] = clean_data['Model Year'].fillna(\"Missing\")\n",
    "\n",
    "    # fill missing CPI's with average CPI\n",
    "    clean_data['CPI'] = clean_data['CPI'].fillna(clean_data['CPI'].mean())\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_training = clean_input_data(training)\n",
    "clean_scoring = clean_input_data(scoring)\n",
    "\n",
    "car_data = pd.concat([clean_training, clean_scoring]).reset_index(drop=True)\n",
    "car_data['Model Year'] = car_data['Model Year'].astype(str)\n",
    "\n",
    "categorical_cols = car_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_car_data =  pd.get_dummies(car_data, columns=categorical_cols)\n",
    "\n",
    "train_indices = range(0, len(clean_training))\n",
    "test_indices = range(len(clean_training), len(car_data))\n",
    "\n",
    "train = car_data.loc[train_indices].copy()\n",
    "test = car_data.loc[test_indices].copy()\n",
    "\n",
    "train_encoded = encoded_car_data.loc[train_indices].copy()\n",
    "test_encoded = encoded_car_data.loc[test_indices].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "class ConstantModel:\n",
    "    # A fallback model that always predicts a constant value\n",
    "    def __init__(self, value):\n",
    "        self.value = value  # Store the constant target value\n",
    "\n",
    "    def predict(self, X):\n",
    "        #  mimic contant\n",
    "        if not hasattr(X, '__len__'):  # Ensure X has a length (is iterable)\n",
    "            return self.value  # Just return the value if X isn't iterable\n",
    "        return [self.value] * len(X)  # Normal case\n",
    "\n",
    "def fit_catboost(X, y):\n",
    "    try:\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=100,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            cat_features=list(X.columns),\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        model.fit(X, y)\n",
    "\n",
    "        return model\n",
    "    except:\n",
    "        singular_value = y.iloc[0] if hasattr(y, 'iloc') else y[0]  # Extract the constant target value\n",
    "\n",
    "        return ConstantModel(singular_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_options = list(set(train.columns) - {'Date', 'Vehicle Population', 'Vehicle Category'})\n",
    "\n",
    "basis_prediction_combos = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    # Always include 'Vehicle Category'\n",
    "    basis_columns = ['Vehicle Category']\n",
    "    \n",
    "    additional_basis_columns = random.sample(column_options, random.randint(0, 2))\n",
    "    basis_columns += additional_basis_columns\n",
    "\n",
    "    remaining_columns = list(set(column_options) - set(additional_basis_columns))\n",
    "    prediction_columns = random.sample(remaining_columns, random.randint(3, 8 - len(basis_columns)))\n",
    "\n",
    "    basis_prediction_combos.append((basis_columns, prediction_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian hyperparameter optimization framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y, cat_features):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),                    \n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),     \n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),                  \n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 1),            \n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),  \n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),           \n",
    "        'verbose': 0\n",
    "    }\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    model = CatBoostRegressor(**params, cat_features=cat_features)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    preds = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "def optimize_catboost(X, y, cat_features, n_trials=20):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, X, y, cat_features), n_trials=n_trials, timeout=30)\n",
    "    best_params = study.best_params\n",
    "\n",
    "    model = CatBoostRegressor(**best_params, cat_features=cat_features)\n",
    "    model.fit(X, y, verbose=0)\n",
    "    return model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis models workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_by_basis(train_data, train_encoded, basis_columns, prediction_columns, target_col, model_func, encode):\n",
    "\n",
    "    basis_values = [train_data[col].unique().tolist() for col in basis_columns]\n",
    "    basis_combinations = [tuple(values) for values in product(*basis_values)]\n",
    "\n",
    "    cat_features = [col for col in prediction_columns if train_data[col].dtype == 'object']\n",
    "\n",
    "    if not cat_features:\n",
    "        cat_features = []\n",
    "\n",
    "    trained_models = {}\n",
    "    for basis_key in basis_combinations:\n",
    "        basis_dict = {col: value for col, value in zip(basis_columns, basis_key)}\n",
    "        \n",
    "        basis_indices = train_data[\n",
    "            train_data[list(basis_dict.keys())].eq(pd.Series(basis_dict)).all(axis=1)\n",
    "        ].index\n",
    "\n",
    "        model_data = train_encoded.copy() if encode else train_data.copy()\n",
    "        X = model_data.loc[basis_indices, prediction_columns]\n",
    "        y = model_data.loc[basis_indices, target_col]\n",
    "\n",
    "        if len(X) > 10:  # Sufficient data\n",
    "            model, params = optimize_catboost(X, y, cat_features)\n",
    "            safe = True\n",
    "        else:  # Fallback model for small datasets\n",
    "            model = model_func(model_data[prediction_columns], model_data[target_col])\n",
    "            params = None\n",
    "            safe = False\n",
    "\n",
    "        trained_models[basis_key] = (model, safe, params)\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "\n",
    "def calculate_rmse(test_data, encoded_test_data, trained_models, basis_columns, prediction_columns, target_col, encode):\n",
    "    all_predictions = []\n",
    "\n",
    "    for idx, row in test_data.iterrows():\n",
    "        model_basis_key = tuple(row[basis_columns])\n",
    "        model_info = trained_models.get(model_basis_key, (None, False, None))\n",
    "        model, safe, params = model_info\n",
    "\n",
    "        if model:\n",
    "            formatted_row = (encoded_test_data if encode else test_data).loc[idx, prediction_columns]\n",
    "            prediction = model.predict(pd.DataFrame(formatted_row).T)[0]\n",
    "            actual = row[target_col]\n",
    "\n",
    "            all_predictions.append({\n",
    "                'index': idx,\n",
    "                'prediction': round(prediction, 0),\n",
    "                'actual': actual,\n",
    "                'basis_key': model_basis_key,\n",
    "                'safe': safe\n",
    "            })\n",
    "\n",
    "    if all_predictions:\n",
    "        pred_df = pd.DataFrame(all_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(pred_df['actual'], pred_df['prediction']))\n",
    "    else:\n",
    "        rmse = None  # No predictions made\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and optimize basis subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/Users/teymour/Desktop/Datathon/results/catboost_optimization.xlsx'\n",
    "\n",
    "def append_to_excel(df, file_path):\n",
    "    \"\"\"\n",
    "    Appends DataFrame `df` to the Excel file at `file_path`.\n",
    "    Creates the file if it doesn't exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        df.to_excel(file_path, index=False)\n",
    "    else:\n",
    "        with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "            workbook = load_workbook(file_path)\n",
    "            sheet = workbook.active\n",
    "            startrow = sheet.max_row\n",
    "            \n",
    "            df.to_excel(writer, index=False, header=False, startrow=startrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = fit_catboost\n",
    "column_options = list(set(train.columns) - {'Date', 'Vehicle Population'})\n",
    "encode = False\n",
    "\n",
    "def process_basis_prediction_combo(basis_columns, prediction_columns):\n",
    "    try:\n",
    "        target_col = 'Vehicle Population'\n",
    "        \n",
    "        clean_prediction_columns = (\n",
    "            [col for col in train_encoded.columns if col.split(\"_\")[0] not in basis_columns and col not in [target_col, 'Date']]\n",
    "            if encode else prediction_columns\n",
    "        )\n",
    "        \n",
    "        trained_models = get_models_by_basis(train, train_encoded, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n",
    "        \n",
    "        rmse = calculate_rmse(test, test_encoded, trained_models, basis_columns, clean_prediction_columns, target_col, encode)\n",
    "\n",
    "        basis_keys_params = {\n",
    "            basis_key: params for basis_key, (_, _, params) in trained_models.items()\n",
    "        }\n",
    "\n",
    "        results = pd.DataFrame({\n",
    "            'basis_columns': [basis_columns],\n",
    "            'basis_keys_used': [list(trained_models.keys())],\n",
    "            'basis_keys_params': [basis_keys_params],\n",
    "            'prediction_columns': [prediction_columns],\n",
    "            'rmse': [rmse]\n",
    "        })\n",
    "\n",
    "        print(results) \n",
    "\n",
    "        append_to_excel(results, output_path)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing combo {basis_columns}, {prediction_columns}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "model_results_list = []\n",
    "with tqdm(desc=\"Processing Models\", total=len(basis_prediction_combos)) as pbar:\n",
    "    model_results_list = Parallel(n_jobs=-1)(\n",
    "        delayed(process_basis_prediction_combo)(basis, pred) for basis, pred in basis_prediction_combos\n",
    "    )\n",
    "    pbar.update(len(basis_prediction_combos))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
