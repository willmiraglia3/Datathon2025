{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PolynomialFeatures was fitted with feature names\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Data Dictionary\n",
    "data_dict_path = '../../data/data_dictionary.xlsx'\n",
    "data_dictionary = pd.read_excel(data_dict_path)\n",
    "\n",
    "# Download Scoring File -- what the input data will be for the end\n",
    "scoring_path = '../../data/scoring.xlsx'\n",
    "scoring = pd.read_excel(scoring_path)\n",
    "\n",
    "# Download Submission Format -- what the format of the output should be from my model\n",
    "submission_format_path = '../../data/submission_format.csv'\n",
    "submission_format = pd.read_csv(submission_format_path)\n",
    "\n",
    "# Download Training Data -- what the training data is for this\n",
    "training_path = '../../data/training.xlsx'\n",
    "training = pd.read_excel(training_path).dropna()\n",
    "\n",
    "# new data from Fred: https://fred.stlouisfed.org/series/CUSR0000SETA02\n",
    "cpi_used_cars_path = '../../data/CPI_UsedCars_US.xlsx'\n",
    "cpi_used_cars = pd.read_excel(cpi_used_cars_path, sheet_name='Monthly')\n",
    "cpi_used_cars.columns = ['Date', 'CPI']\n",
    "cpi_used_cars['Year'] = cpi_used_cars['Date'].dt.year\n",
    "average_cpi_by_year = cpi_used_cars.groupby('Year')['CPI'].mean().reset_index().rename(columns={'Year': 'Model Year'})\n",
    "# since average cpi by year is missing 2025 and 2026, forecast using a neural prophet model\n",
    "def forecast_cpi_polynomial(average_cpi_by_year, forecast_periods=5, degree=2):\n",
    "    # Ensure data is sorted by 'Model Year'\n",
    "    average_cpi_by_year = average_cpi_by_year.sort_values(by='Model Year')\n",
    "\n",
    "    # Extract features and target variable\n",
    "    X = average_cpi_by_year[['Model Year']]\n",
    "    y = average_cpi_by_year['CPI']\n",
    "\n",
    "    # Generate polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    # Fit the polynomial regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "\n",
    "    # Generate future 'Model Year' values\n",
    "    last_year = X['Model Year'].max()\n",
    "    future_years = np.arange(last_year + 1, last_year + 1 + forecast_periods).reshape(-1, 1)\n",
    "\n",
    "    # Transform future years to polynomial features\n",
    "    future_years_poly = poly.transform(future_years)\n",
    "\n",
    "    # Predict future CPI values\n",
    "    future_cpi = model.predict(future_years_poly)\n",
    "\n",
    "    # Create DataFrame for future predictions\n",
    "    future_df = pd.DataFrame({\n",
    "        'Model Year': future_years.flatten(),\n",
    "        'CPI': future_cpi\n",
    "    })\n",
    "\n",
    "    # Combine original and forecasted data\n",
    "    combined_df = pd.concat([average_cpi_by_year, future_df], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "average_cpi_by_year = forecast_cpi_polynomial(average_cpi_by_year, 2, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input_data(data):\n",
    "    # add the CPI of Used Cars by City Average\n",
    "    clean_data = data.merge(average_cpi_by_year, on='Model Year', how='left')\n",
    "    clean_data['Model Year'] = clean_data['Model Year'].astype(str)\n",
    "    \n",
    "    # handle nulls\n",
    "    clean_data = clean_data.replace({\"nan\": np.nan})\n",
    "    clean_data['Model Year'] = clean_data['Model Year'].fillna(\"Missing\")\n",
    "\n",
    "    # fill missing CPI's with average CPI\n",
    "    clean_data['CPI'] = clean_data['CPI'].fillna(clean_data['CPI'].mean())\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "class ConstantModel:\n",
    "    # A fallback model that always predicts a constant value\n",
    "    def __init__(self, value):\n",
    "        self.value = value  # Store the constant target value\n",
    "\n",
    "    def predict(self, X):\n",
    "        #  mimic contant\n",
    "        if not hasattr(X, '__len__'):  # Ensure X has a length (is iterable)\n",
    "            return self.value  # Just return the value if X isn't iterable\n",
    "        return [self.value] * len(X)  # Normal case\n",
    "\n",
    "\n",
    "def fit_catboost(X, y):\n",
    "    try:\n",
    "\n",
    "        categorical_cols = list(X.columns)\n",
    "        if 'CPI' in categorical_cols:\n",
    "            categorical_cols.remove('CPI')\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=100,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            cat_features=categorical_cols,\n",
    "            random_seed=42,\n",
    "            allow_writing_files=False,  # Prevents CatBoost from creating the catboost_info directory\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Clear CatBoost temporary files\n",
    "        catboost_tmp_dir = tempfile.gettempdir()  # Get system temp dir\n",
    "        for root, dirs, files in os.walk(catboost_tmp_dir):\n",
    "            for dir_name in dirs:\n",
    "                if \"catboost\" in dir_name:\n",
    "                    shutil.rmtree(os.path.join(root, dir_name), ignore_errors=True)\n",
    "\n",
    "        return model\n",
    "    except:\n",
    "        singular_value = y.iloc[0] if hasattr(y, 'iloc') else y[0]\n",
    "        return ConstantModel(singular_value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache_and_temp_files(root_dir='.'):\n",
    "    \"\"\"\n",
    "    Recursively deletes __pycache__ directories and .pyc files starting from the root_dir.\n",
    "    \"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # Remove __pycache__ directories\n",
    "        if '__pycache__' in dirnames:\n",
    "            pycache_path = os.path.join(dirpath, '__pycache__')\n",
    "            shutil.rmtree(pycache_path, ignore_errors=True)\n",
    "            print(f\"Deleted directory: {pycache_path}\")\n",
    "        \n",
    "        # Remove .pyc files\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.pyc'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted file: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_training = clean_input_data(training)\n",
    "clean_scoring = clean_input_data(scoring)\n",
    "\n",
    "car_data = pd.concat([clean_training, clean_scoring]).reset_index(drop=True)\n",
    "car_data['Model Year'] = car_data['Model Year'].astype(str)\n",
    "\n",
    "categorical_cols = car_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_car_data =  pd.get_dummies(car_data, columns=categorical_cols)\n",
    "\n",
    "train_indices = range(0, len(clean_training))\n",
    "test_indices = range(len(clean_training), len(car_data))\n",
    "\n",
    "train = car_data.loc[train_indices].copy()\n",
    "test = car_data.loc[test_indices].copy()\n",
    "\n",
    "train_encoded = encoded_car_data.loc[train_indices].copy()\n",
    "test_encoded = encoded_car_data.loc[test_indices].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_by_basis(car_data, encoded_car_data, train_indices, basis_columns, prediction_columns, target_col, model_func, encode):\n",
    "\n",
    "    train_data = car_data.loc[train_indices]\n",
    "    encoded_train = encoded_car_data.loc[train_indices]\n",
    "\n",
    "    # Define the columns used to create different models\n",
    "    basis_values = [train_data[col].unique().tolist() for col in basis_columns] # these are the potential values for each basis\n",
    "\n",
    "    # Generate all possible (Model Year, Fuel Type) combinations as tuples\n",
    "    basis_combinations = [tuple(values) for values in product(*basis_values)] # these are the combos of basis's so like 2020 Electric for example\n",
    "\n",
    "    # Dictionary to store trained models, using (Model Year, Fuel Type) as the key\n",
    "    trained_models = {}\n",
    "    for basis_key in basis_combinations:\n",
    "\n",
    "        basis_dict = {col: value for col, value in zip(basis_columns, basis_key)}\n",
    "        # Filter training data for the specific Model Year & Fuel Type\n",
    "        basis_indices = train_data[\n",
    "            train_data[basis_dict.keys()].eq(pd.Series(basis_dict)).all(axis=1)\n",
    "        ].index \n",
    "\n",
    "        # check if encoded\n",
    "        model_data = encoded_train.copy() if encode else train_data.copy()\n",
    "\n",
    "        # initialize X and y\n",
    "        X = model_data.loc[basis_indices, prediction_columns] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "        y = model_data.loc[basis_indices][target_col] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "\n",
    "\n",
    "        if len(X) > 10: # if there are at least 10 values\n",
    "            model = model_func(X, y)\n",
    "            safe = True\n",
    "        else:\n",
    "            model = model_func(model_data[prediction_columns], model_data[target_col])\n",
    "            safe = False\n",
    "\n",
    "        # Store trained model using the (Model Year, Fuel Type) tuple as the key\n",
    "        trained_models[basis_key] = (model, safe)\n",
    "\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, prediction_columns, target_col, encode):\n",
    "    all_prediction_rows = []\n",
    "    test = car_data.loc[test_indices]\n",
    "\n",
    "    for test_idx in test_indices:\n",
    "\n",
    "        row = car_data.loc[test_idx]\n",
    "\n",
    "        # get the basis values in the row (like 2020 for Model Year if Model Year is a basis)\n",
    "        model_basis_dict = {col: row[col] for col in basis_columns}\n",
    "\n",
    "        # get the model based on the basis\n",
    "        model, safe = trained_models[tuple(model_basis_dict.values())]\n",
    "\n",
    "        # get the encoded row\n",
    "        if encode:\n",
    "            formatted_row = encoded_car_data.loc[test_idx]\n",
    "            formatted_row = formatted_row[prediction_columns]\n",
    "        else:\n",
    "            formatted_row = row[prediction_columns]\n",
    "\n",
    "        # get the prediction vs. actual\n",
    "        prediction = model.predict(pd.DataFrame(formatted_row).T)[0]\n",
    "        actual = test.loc[test_idx, target_col]\n",
    "\n",
    "        prediction_row = pd.DataFrame({\n",
    "            'index': [test_idx],\n",
    "            'prediction': [round(prediction, 0)],\n",
    "            'actual': [actual],\n",
    "            'safe': [safe]\n",
    "        })\n",
    "        all_prediction_rows.append(prediction_row)\n",
    "\n",
    "    prediction_df = pd.concat(all_prediction_rows)\n",
    "    rmse = np.sqrt(mean_squared_error(prediction_df['actual'], prediction_df['prediction']))\n",
    "\n",
    "    return rmse, prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i>Get the combinations of basis columns and prediction columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_options = list(set(car_data.columns) - {'Date', 'Vehicle Population'})\n",
    "\n",
    "basis_prediction_combos = []\n",
    "for _ in range(250):\n",
    "    # Select 1-3 random basis columns\n",
    "    basis_columns = random.sample(column_options, random.randint(1, 2))\n",
    "    if 'CPI' in basis_columns:\n",
    "        basis_columns.remove('CPI')\n",
    "    if len(basis_columns) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Select 3-7 prediction columns that are **not in basis_columns**\n",
    "    remaining_columns = list(set(column_options) - set(basis_columns))\n",
    "    prediction_columns = random.sample(remaining_columns, random.randint(3, 8-len(basis_columns)))\n",
    "    \n",
    "    # Store as a tuple\n",
    "    basis_prediction_combos.append((basis_columns, prediction_columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i>Run the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to process each set of columns\n",
    "model_func = fit_linear_regression\n",
    "encode = True\n",
    "\n",
    "# Define file path for saving results\n",
    "results_file = f'../../results/{model_func.__name__}_optim.csv'\n",
    "\n",
    "# Load existing results if the file exists\n",
    "if os.path.exists(results_file):\n",
    "    model_results = pd.read_csv(results_file)\n",
    "else:\n",
    "    model_results = pd.DataFrame(columns=['basis_columns', 'prediction_columns', 'rmse'])\n",
    "\n",
    "# Convert existing basis-prediction combinations to a set for quick lookup\n",
    "existing_combos = set(zip(model_results['basis_columns'].astype(str), model_results['prediction_columns'].astype(str)))\n",
    "\n",
    "target_col = 'Vehicle Population'\n",
    "\n",
    "def process_combo(basis_columns, prediction_columns, save=True):\n",
    "    try:\n",
    "        \n",
    "        # Convert combo to string format for checking existence\n",
    "        combo_key = (str(basis_columns), str(prediction_columns))\n",
    "        if combo_key in existing_combos:\n",
    "            print(f\"Skipping already processed: {basis_columns}, {prediction_columns}\")\n",
    "            return None\n",
    "\n",
    "        # Ensure prediction columns do not include basis columns\n",
    "        if encode:\n",
    "            clean_prediction_columns = list(set(col for col in train_encoded.columns if col.split(\"_\")[0] not in basis_columns) - {target_col, 'Date'})\n",
    "        else:\n",
    "            clean_prediction_columns = prediction_columns\n",
    "        \n",
    "        # Train models and calculate RMSE\n",
    "        trained_models = get_models_by_basis(car_data, encoded_car_data, train_indices, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n",
    "        rmse = calculate_test_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, clean_prediction_columns, target_col, encode)[0]\n",
    "\n",
    "        # Create DataFrame for this iteration\n",
    "        result_df = pd.DataFrame({\n",
    "            'basis_columns': [basis_columns],\n",
    "            'prediction_columns': [prediction_columns],\n",
    "            'rmse': [rmse]\n",
    "        })\n",
    "\n",
    "        if save:\n",
    "            # Append results to CSV immediately\n",
    "            result_df.to_csv(results_file, index=False, mode='a', header=not os.path.exists(results_file))\n",
    "\n",
    "        try:\n",
    "            shutil.rmtree('catboost_info')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "with tqdm_joblib(tqdm(desc=\"Processing Models\", total=len(basis_prediction_combos))) as progress_bar:\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_combo)(basis, pred) for basis, pred in basis_prediction_combos\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iterations': 853, 'learning_rate': 0.2429781556456842, 'depth': 6, 'l2_leaf_reg': 2.282136581605469, 'bagging_temperature': 0.21448680294505562, 'border_count': 202, 'random_strength': 0.5230550908979421, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 8}\n",
      "{'iterations': 335, 'learning_rate': 0.26931213330418335, 'depth': 10, 'l2_leaf_reg': 9.640870180295874, 'bagging_temperature': 0.30115087324036427, 'border_count': 178, 'random_strength': 0.39990182837262866, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 14}\n",
      "{'iterations': 865, 'learning_rate': 0.1010367015771094, 'depth': 9, 'l2_leaf_reg': 7.42764752331621, 'bagging_temperature': 0.6459945532391831, 'border_count': 136, 'random_strength': 0.3485326296696091, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 19}\n",
      "{'iterations': 171, 'learning_rate': 0.10839852850884317, 'depth': 7, 'l2_leaf_reg': 6.465152613240907, 'bagging_temperature': 0.05956399621682334, 'border_count': 32, 'random_strength': 0.9969824754272121, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 1}\n",
      "{'iterations': 493, 'learning_rate': 0.017972336996238937, 'depth': 5, 'l2_leaf_reg': 9.904395136404345, 'bagging_temperature': 0.003721695868538509, 'border_count': 119, 'random_strength': 0.9731604471489881, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 15}\n",
      "{'iterations': 293, 'learning_rate': 0.16845525328753766, 'depth': 9, 'l2_leaf_reg': 7.3028486840112725, 'bagging_temperature': 0.1990811649266926, 'border_count': 84, 'random_strength': 0.8609406400707353, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 6}\n",
      "{'iterations': 801, 'learning_rate': 0.08299420642146778, 'depth': 9, 'l2_leaf_reg': 1.9216760380872735, 'bagging_temperature': 0.0009397731362831428, 'border_count': 213, 'random_strength': 0.19396619269250526, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 16}\n",
      "{'iterations': 610, 'learning_rate': 0.12293705953473834, 'depth': 8, 'l2_leaf_reg': 4.96309503860056, 'bagging_temperature': 0.873646019286267, 'border_count': 191, 'random_strength': 0.8196419736475136, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 9}\n",
      "{'iterations': 563, 'learning_rate': 0.21256793340510682, 'depth': 7, 'l2_leaf_reg': 6.892637478520299, 'bagging_temperature': 0.9724910160820344, 'border_count': 191, 'random_strength': 0.4347428589259234, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 9}\n",
      "{'iterations': 885, 'learning_rate': 0.26170267256322144, 'depth': 8, 'l2_leaf_reg': 6.994752208188576, 'bagging_temperature': 0.9067775235329057, 'border_count': 149, 'random_strength': 0.29522679258872647, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 13}\n",
      "{'iterations': 765, 'learning_rate': 0.0863404017910146, 'depth': 4, 'l2_leaf_reg': 6.859286521399906, 'bagging_temperature': 0.7652808535846216, 'border_count': 141, 'random_strength': 0.6183738585814146, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 6}\n",
      "{'iterations': 272, 'learning_rate': 0.23433232647434127, 'depth': 5, 'l2_leaf_reg': 3.0329940707082588, 'bagging_temperature': 0.13616676011461504, 'border_count': 238, 'random_strength': 0.7781224191172322, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 18}\n",
      "{'iterations': 768, 'learning_rate': 0.2575430567722977, 'depth': 4, 'l2_leaf_reg': 1.6541315061037025, 'bagging_temperature': 0.628596056037448, 'border_count': 153, 'random_strength': 0.6865879212812991, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 6}\n"
     ]
    }
   ],
   "source": [
    "basis_column = 'Vehicle Type'\n",
    "\n",
    "basis_keys = [('P',), ('T2',), ('T3',), ('T1',), ('MC',), ('T4',), ('T7',), ('T6',), ('T5',), ('MH',), ('BS',), ('B',), ('BT',)]\n",
    "\n",
    "params_dict = {\n",
    "    ('P',): {'iterations': 853, 'learning_rate': 0.2429781556456842, 'depth': 6, 'l2_leaf_reg': 2.282136581605469, 'bagging_temperature': 0.21448680294505562, 'border_count': 202, 'random_strength': 0.5230550908979421, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 8},\n",
    "    ('T2',): {'iterations': 335, 'learning_rate': 0.26931213330418335, 'depth': 10, 'l2_leaf_reg': 9.640870180295874, 'bagging_temperature': 0.30115087324036427, 'border_count': 178, 'random_strength': 0.39990182837262866, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 14},\n",
    "    ('T3',): {'iterations': 865, 'learning_rate': 0.1010367015771094, 'depth': 9, 'l2_leaf_reg': 7.42764752331621, 'bagging_temperature': 0.6459945532391831, 'border_count': 136, 'random_strength': 0.3485326296696091, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 19},\n",
    "    ('T1',): {'iterations': 171, 'learning_rate': 0.10839852850884317, 'depth': 7, 'l2_leaf_reg': 6.465152613240907, 'bagging_temperature': 0.05956399621682334, 'border_count': 32, 'random_strength': 0.9969824754272121, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 1},\n",
    "    ('MC',): {'iterations': 493, 'learning_rate': 0.017972336996238937, 'depth': 5, 'l2_leaf_reg': 9.904395136404345, 'bagging_temperature': 0.003721695868538509, 'border_count': 119, 'random_strength': 0.9731604471489881, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 15},\n",
    "    ('T4',): {'iterations': 293, 'learning_rate': 0.16845525328753766, 'depth': 9, 'l2_leaf_reg': 7.3028486840112725, 'bagging_temperature': 0.1990811649266926, 'border_count': 84, 'random_strength': 0.8609406400707353, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 6},\n",
    "    ('T7',): {'iterations': 801, 'learning_rate': 0.08299420642146778, 'depth': 9, 'l2_leaf_reg': 1.9216760380872735, 'bagging_temperature': 0.0009397731362831428, 'border_count': 213, 'random_strength': 0.19396619269250526, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 16},\n",
    "    ('T6',): {'iterations': 610, 'learning_rate': 0.12293705953473834, 'depth': 8, 'l2_leaf_reg': 4.96309503860056, 'bagging_temperature': 0.873646019286267, 'border_count': 191, 'random_strength': 0.8196419736475136, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 9},\n",
    "    ('T5',): {'iterations': 563, 'learning_rate': 0.21256793340510682, 'depth': 7, 'l2_leaf_reg': 6.892637478520299, 'bagging_temperature': 0.9724910160820344, 'border_count': 191, 'random_strength': 0.4347428589259234, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 9},\n",
    "    ('MH',): {'iterations': 885, 'learning_rate': 0.26170267256322144, 'depth': 8, 'l2_leaf_reg': 6.994752208188576, 'bagging_temperature': 0.9067775235329057, 'border_count': 149, 'random_strength': 0.29522679258872647, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 13},\n",
    "    ('BS',): {'iterations': 765, 'learning_rate': 0.0863404017910146, 'depth': 4, 'l2_leaf_reg': 6.859286521399906, 'bagging_temperature': 0.7652808535846216, 'border_count': 141, 'random_strength': 0.6183738585814146, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 6},\n",
    "    ('B',): {'iterations': 272, 'learning_rate': 0.23433232647434127, 'depth': 5, 'l2_leaf_reg': 3.0329940707082588, 'bagging_temperature': 0.13616676011461504, 'border_count': 238, 'random_strength': 0.7781224191172322, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 18},\n",
    "    ('BT',): {'iterations': 768, 'learning_rate': 0.2575430567722977, 'depth': 4, 'l2_leaf_reg': 1.6541315061037025, 'bagging_temperature': 0.628596056037448, 'border_count': 153, 'random_strength': 0.6865879212812991, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 6}\n",
    "}\n",
    "\n",
    "train_data = car_data.loc[train_indices]\n",
    "encoded_train = encoded_car_data.loc[train_indices]\n",
    "\n",
    "# Define the columns used to create different models\n",
    "basis_values = [train_data[col].unique().tolist() for col in basis_columns] # these are the potential values for each basis\n",
    "\n",
    "# Generate all possible (Model Year, Fuel Type) combinations as tuples\n",
    "basis_combinations = [tuple(values) for values in product(*basis_values)] # these are the combos of basis's so like 2020 Electric for example\n",
    "\n",
    "# Dictionary to store trained models, using (Model Year, Fuel Type) as the key\n",
    "trained_models = {}\n",
    "for basis_key in basis_combinations:\n",
    "\n",
    "    optimal_params = params_dict[basis_key]\n",
    "\n",
    "    basis_dict = {col: value for col, value in zip(basis_columns, basis_key)}\n",
    "    # Filter training data for the specific Model Year & Fuel Type\n",
    "    basis_indices = train_data[\n",
    "        train_data[basis_dict.keys()].eq(pd.Series(basis_dict)).all(axis=1)\n",
    "    ].index \n",
    "\n",
    "    # check if encoded\n",
    "    model_data = encoded_train.copy() if encode else train_data.copy()\n",
    "\n",
    "    # initialize X and y\n",
    "    X = model_data.loc[basis_indices, prediction_columns] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "    y = model_data.loc[basis_indices][target_col] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "\n",
    "    model = CatBoostRegressor(*optimal_params)\n",
    "    if len(X) > 10: # if there are at least 10 values\n",
    "        model.fit(X, y)\n",
    "        safe = True\n",
    "    else:\n",
    "        model.fit(model_data[prediction_columns], model_data[target_col])\n",
    "        safe = False\n",
    "\n",
    "    # Store trained model using the (Model Year, Fuel Type) tuple as the key\n",
    "    trained_models[basis_key] = (model, safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:   0%|          | 0/1 [00:53<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9160.065809717278"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_scoring =  encoded_car_data.loc[test_indices]\n",
    "\n",
    "# initialize inputs\n",
    "model_func = fit_catboost\n",
    "encode = False\n",
    "basis_columns = ['Vehicle Category', 'Fuel Type']\n",
    "prediction_columns = ['GVWR Class', 'CPI', 'Number of Vehicles Registered at the Same Address', 'Model Year', 'Fuel Technology']\n",
    "target_col = 'Vehicle Population'\n",
    "\n",
    "# clean prediction columns if necessary\n",
    "if encode:\n",
    "    clean_prediction_columns = list(set(col for col in encoded_scoring.columns if col.split(\"_\")[0] not in basis_columns) - {target_col, 'Date'})\n",
    "else:\n",
    "    clean_prediction_columns = prediction_columns\n",
    "\n",
    "trained_models = get_models_by_basis(car_data, encoded_car_data, train_indices, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n",
    "rmse, pred_df = calculate_test_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, clean_prediction_columns, target_col, encode)\n",
    "\n",
    "submission = pred_df[['prediction']].rename(columns={'prediction': 'Vehicle Population'}).reset_index(drop=True)\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
