{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/tqdm_joblib/__init__.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data Dictionary\n",
    "data_dict_path = '../../data/data_dictionary.xlsx'\n",
    "data_dictionary = pd.read_excel(data_dict_path)\n",
    "\n",
    "# Download Scoring File -- what the input data will be for the end\n",
    "scoring_path = '../../data/scoring.xlsx'\n",
    "scoring = pd.read_excel(scoring_path)\n",
    "\n",
    "# Download Submission Format -- what the format of the output should be from my model\n",
    "submission_format_path = '../../data/submission_format.csv'\n",
    "submission_format = pd.read_csv(submission_format_path)\n",
    "\n",
    "# Download Training Data -- what the training data is for this\n",
    "training_path = '../../data/training.xlsx'\n",
    "training = pd.read_excel(training_path).dropna()\n",
    "\n",
    "# new data from Fred: https://fred.stlouisfed.org/series/CUSR0000SETA02\n",
    "cpi_used_cars_path = '../../data/CPI_UsedCars_US.xlsx'\n",
    "cpi_used_cars = pd.read_excel(cpi_used_cars_path, sheet_name='Monthly')\n",
    "cpi_used_cars.columns = ['Date', 'CPI']\n",
    "cpi_used_cars['Year'] = cpi_used_cars['Date'].dt.year\n",
    "average_cpi_by_year = cpi_used_cars.groupby('Year')['CPI'].mean().reset_index().rename(columns={'Year': 'Model Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input_data(data):\n",
    "    # add the CPI of Used Cars by City Average\n",
    "    clean_data = data.merge(average_cpi_by_year, on='Model Year', how='left').dropna()\n",
    "    clean_data['Model Year'] = clean_data['Model Year'].astype(str)\n",
    "    return clean_data\n",
    "\n",
    "training = clean_input_data(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y, groups=None):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "class ConstantModel:\n",
    "    # A fallback model that always predicts a constant value\n",
    "    def __init__(self, value):\n",
    "        self.value = value  # Store the constant target value\n",
    "\n",
    "    def predict(self, X, groups=None):\n",
    "        #  mimic contant\n",
    "        if not hasattr(X, '__len__'):  # Ensure X has a length (is iterable)\n",
    "            return self.value  # Just return the value if X isn't iterable\n",
    "        return [self.value] * len(X)  # Normal case\n",
    "\n",
    "\n",
    "def fit_catboost(X, y, groups=None):\n",
    "    try:\n",
    "\n",
    "        categorical_cols = list(X.columns)\n",
    "        if 'CPI' in categorical_cols:\n",
    "            categorical_cols.remove('CPI')\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=100,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            cat_features=categorical_cols,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Clear CatBoost temporary files\n",
    "        catboost_tmp_dir = tempfile.gettempdir()  # Get system temp dir\n",
    "        for root, dirs, files in os.walk(catboost_tmp_dir):\n",
    "            for dir_name in dirs:\n",
    "                if \"catboost\" in dir_name:\n",
    "                    shutil.rmtree(os.path.join(root, dir_name), ignore_errors=True)\n",
    "\n",
    "        return model\n",
    "    except:\n",
    "        singular_value = y.iloc[0] if hasattr(y, 'iloc') else y[0]\n",
    "        return ConstantModel(singular_value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data = training.copy()\n",
    "\n",
    "categorical_cols = car_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_car_data =  pd.get_dummies(car_data, columns=categorical_cols)\n",
    "\n",
    "train_indices = car_data.sample(frac=0.8, random_state=0).index\n",
    "test_indices = car_data.drop(train_indices).index\n",
    "\n",
    "train = car_data.loc[train_indices].copy()\n",
    "test = car_data.loc[test_indices].copy()\n",
    "\n",
    "train_encoded = encoded_car_data.loc[train_indices].copy()\n",
    "test_encoded = encoded_car_data.loc[test_indices].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_by_basis(train_data, train_encoded, basis_columns, prediction_columns, target_col, model_func, encode):\n",
    "\n",
    "    # Define the columns used to create different models\n",
    "    basis_values = [train_data[col].unique().tolist() for col in basis_columns] # these are the potential values for each basis\n",
    "\n",
    "    # Generate all possible (Model Year, Fuel Type) combinations as tuples\n",
    "    basis_combinations = [tuple(values) for values in product(*basis_values)] # these are the combos of basis's so like 2020 Electric for example\n",
    "\n",
    "    # Dictionary to store trained models, using (Model Year, Fuel Type) as the key\n",
    "    trained_models = {}\n",
    "    for basis_key in basis_combinations:\n",
    "\n",
    "        basis_dict = {col: value for col, value in zip(basis_columns, basis_key)}\n",
    "        # Filter training data for the specific Model Year & Fuel Type\n",
    "        basis_indices = train_data[\n",
    "            train_data[basis_dict.keys()].eq(pd.Series(basis_dict)).all(axis=1)\n",
    "        ].index \n",
    "\n",
    "        # check if encoded\n",
    "        model_data = train_encoded.copy() if encode else train_data.copy()\n",
    "\n",
    "        # initialize X and y\n",
    "        X = model_data.loc[basis_indices, prediction_columns]\n",
    "        y = model_data.loc[basis_indices][target_col]\n",
    "\n",
    "\n",
    "        if len(X) > 10: # if there are at least 10 values\n",
    "            model = model_func(X, y)\n",
    "            safe = True\n",
    "        else:\n",
    "            model = model_func(model_data[prediction_columns], model_data[target_col])\n",
    "            safe = False\n",
    "\n",
    "        # Store trained model using the (Model Year, Fuel Type) tuple as the key\n",
    "        trained_models[basis_key] = (model, safe)\n",
    "\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, prediction_columns, target_col, encode):\n",
    "    all_prediction_rows = []\n",
    "    for test_idx in test_indices:\n",
    "\n",
    "        row = car_data.loc[test_idx]\n",
    "\n",
    "        # get the basis values in the row (like 2020 for Model Year if Model Year is a basis)\n",
    "        model_basis_dict = {col: row[col] for col in basis_columns}\n",
    "\n",
    "        # get the model based on the basis\n",
    "        model, safe = trained_models[tuple(model_basis_dict.values())]\n",
    "\n",
    "        # get the encoded row\n",
    "        if encode:\n",
    "            formatted_row = encoded_car_data.loc[test_idx]\n",
    "            formatted_row = formatted_row[prediction_columns]\n",
    "        else:\n",
    "            formatted_row = row[prediction_columns]\n",
    "\n",
    "        # get the prediction vs. actual\n",
    "        prediction = model.predict(pd.DataFrame(formatted_row).T)[0]\n",
    "        actual = test.loc[test_idx, target_col]\n",
    "\n",
    "        prediction_row = pd.DataFrame({\n",
    "            'index': [test_idx],\n",
    "            'prediction': [round(prediction, 0)],\n",
    "            'actual': [actual],\n",
    "            'safe': [safe]\n",
    "        })\n",
    "        all_prediction_rows.append(prediction_row)\n",
    "\n",
    "    prediction_df = pd.concat(all_prediction_rows)\n",
    "    rmse = np.sqrt(mean_squared_error(prediction_df['actual'], prediction_df['prediction']))\n",
    "\n",
    "    del trained_models\n",
    "    gc.collect()\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the combinations of basis columns and prediction columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_options = list(set(train.columns) - {'Date', 'Vehicle Population'})\n",
    "\n",
    "basis_prediction_combos = []\n",
    "for _ in range(250):\n",
    "    # Select 1-3 random basis columns\n",
    "    basis_columns = random.sample(column_options, random.randint(1, 3))\n",
    "    \n",
    "    # Select 3-7 prediction columns that are **not in basis_columns**\n",
    "    remaining_columns = list(set(column_options) - set(basis_columns))\n",
    "    prediction_columns = random.sample(remaining_columns, random.randint(3, 8-len(basis_columns)))\n",
    "    \n",
    "    # Store as a tuple\n",
    "    basis_prediction_combos.append((basis_columns, prediction_columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to process each set of columns\n",
    "model_func = fit_catboost\n",
    "encode = False\n",
    "\n",
    "# Define file path for saving results\n",
    "results_file = '../../results/catboost_optim.csv'\n",
    "\n",
    "# Load existing results if the file exists\n",
    "if os.path.exists(results_file):\n",
    "    model_results = pd.read_csv(results_file)\n",
    "else:\n",
    "    model_results = pd.DataFrame(columns=['basis_columns', 'prediction_columns', 'rmse'])\n",
    "\n",
    "# Convert existing basis-prediction combinations to a set for quick lookup\n",
    "existing_combos = set(zip(model_results['basis_columns'].astype(str), model_results['prediction_columns'].astype(str)))\n",
    "\n",
    "def process_basis_prediction_combo(basis_columns, prediction_columns, save=False):\n",
    "    target_col = 'Vehicle Population'\n",
    "    \n",
    "    # Convert combo to string format for checking existence\n",
    "    combo_key = (str(basis_columns), str(prediction_columns))\n",
    "    if combo_key in existing_combos:\n",
    "        print(f\"Skipping already processed: {basis_columns}, {prediction_columns}\")\n",
    "        return None  # Skip iteration\n",
    "    \n",
    "    # Ensure prediction columns do not include basis columns\n",
    "    if encode:\n",
    "        clean_prediction_columns = list(set(col for col in train_encoded.columns if col.split(\"_\")[0] not in basis_columns) - {target_col, 'Date'})\n",
    "    else:\n",
    "        clean_prediction_columns = prediction_columns\n",
    "    \n",
    "    # Train models and calculate RMSE\n",
    "    trained_models = get_models_by_basis(train, train_encoded, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n",
    "    rmse = calculate_rmse(car_data, encoded_car_data, trained_models, basis_columns, clean_prediction_columns, target_col, encode)\n",
    "\n",
    "    # Create DataFrame for this iteration\n",
    "    result_df = pd.DataFrame({\n",
    "        'basis_columns': [basis_columns],\n",
    "        'prediction_columns': [prediction_columns],\n",
    "        'rmse': [rmse]\n",
    "    })\n",
    "\n",
    "    if save:\n",
    "        # Append results to CSV immediately\n",
    "        result_df.to_csv(results_file, index=False, mode='a', header=not os.path.exists(results_file))\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Run in parallel using tqdm_joblib\n",
    "with tqdm_joblib(tqdm(desc=\"Processing Models\", total=len(basis_prediction_combos))):\n",
    "    model_results_list = Parallel(n_jobs=-1)(\n",
    "        delayed(process_basis_prediction_combo)(basis, pred, save=True) for basis, pred in basis_prediction_combos\n",
    "    )\n",
    "\n",
    "# Remove `None` values from the list (skipped iterations)\n",
    "model_results_list = [df for df in model_results_list if df is not None]\n",
    "\n",
    "# Append all new results to the file\n",
    "if model_results_list:\n",
    "    model_results = pd.concat(model_results_list, ignore_index=True)\n",
    "    model_results.to_csv(results_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "95",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 95",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     clean_prediction_columns \u001b[38;5;241m=\u001b[39m prediction_columns\n\u001b[1;32m     19\u001b[0m trained_models \u001b[38;5;241m=\u001b[39m get_models_by_basis(car_data, encoded_car_data, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n\u001b[0;32m---> 20\u001b[0m rmse \u001b[38;5;241m=\u001b[39m calculate_rmse(scoring_input, encoded_scoring_input, trained_models, basis_columns, clean_prediction_columns, target_col, encode)\n\u001b[1;32m     21\u001b[0m rmse\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mcalculate_rmse\u001b[0;34m(car_data, encoded_car_data, trained_models, basis_columns, prediction_columns, target_col, encode)\u001b[0m\n\u001b[1;32m      2\u001b[0m all_prediction_rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_idx \u001b[38;5;129;01min\u001b[39;00m test_indices:\n\u001b[0;32m----> 5\u001b[0m     row \u001b[38;5;241m=\u001b[39m car_data\u001b[38;5;241m.\u001b[39mloc[test_idx]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# get the basis values in the row (like 2020 for Model Year if Model Year is a basis)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     model_basis_dict \u001b[38;5;241m=\u001b[39m {col: row[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m basis_columns}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1431\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_label(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1381\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mxs(label, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4301\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4299\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[1;32m   4300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4301\u001b[0m     loc \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   4304\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 95"
     ]
    }
   ],
   "source": [
    "scoring_input = clean_input_data(scoring)\n",
    "categorical_cols = scoring_input.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_scoring_input =  pd.get_dummies(scoring_input, columns=categorical_cols)\n",
    "\n",
    "# initialize inputs\n",
    "model_func = fit_catboost\n",
    "encode = False\n",
    "basis_columns = ['Vehicle Category', 'Fuel Type']\n",
    "prediction_columns = ['GVWR Class', 'CPI', 'Number of Vehicles Registered at the Same Address', 'Model Year', 'Fuel Technology']\n",
    "target_col = 'Vehicle Population'\n",
    "\n",
    "# clean prediction columns if necessary\n",
    "if encode:\n",
    "    clean_prediction_columns = list(set(col for col in encoded_scoring_input.columns if col.split(\"_\")[0] not in basis_columns) - {target_col, 'Date'})\n",
    "else:\n",
    "    clean_prediction_columns = prediction_columns\n",
    "\n",
    "\n",
    "trained_models = get_models_by_basis(car_data, encoded_car_data, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n",
    "rmse = calculate_rmse(scoring_input, encoded_scoring_input, trained_models, basis_columns, clean_prediction_columns, target_col, encode)\n",
    "rmse\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
