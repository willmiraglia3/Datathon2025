{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from catboost import CatBoostRegressor\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PolynomialFeatures was fitted with feature names\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Data Dictionary\n",
    "data_dict_path = '../../data/data_dictionary.xlsx'\n",
    "data_dictionary = pd.read_excel(data_dict_path)\n",
    "\n",
    "# Download Scoring File -- what the input data will be for the end\n",
    "scoring_path = '../../data/scoring.xlsx'\n",
    "scoring = pd.read_excel(scoring_path)\n",
    "\n",
    "# Download Submission Format -- what the format of the output should be from my model\n",
    "submission_format_path = '../../data/submission_format.csv'\n",
    "submission_format = pd.read_csv(submission_format_path)\n",
    "\n",
    "# Download Training Data -- what the training data is for this\n",
    "training_path = '../../data/training.xlsx'\n",
    "training = pd.read_excel(training_path).dropna()\n",
    "\n",
    "# new data from Fred: https://fred.stlouisfed.org/series/CUSR0000SETA02\n",
    "cpi_used_cars_path = '../../data/CPI_UsedCars_US.xlsx'\n",
    "cpi_used_cars = pd.read_excel(cpi_used_cars_path, sheet_name='Monthly')\n",
    "cpi_used_cars.columns = ['Date', 'CPI']\n",
    "cpi_used_cars['Year'] = cpi_used_cars['Date'].dt.year\n",
    "average_cpi_by_year = cpi_used_cars.groupby('Year')['CPI'].mean().reset_index().rename(columns={'Year': 'Model Year'})\n",
    "# since average cpi by year is missing 2025 and 2026, forecast using a neural prophet model\n",
    "def forecast_cpi_polynomial(average_cpi_by_year, forecast_periods=5, degree=2):\n",
    "    # Ensure data is sorted by 'Model Year'\n",
    "    average_cpi_by_year = average_cpi_by_year.sort_values(by='Model Year')\n",
    "\n",
    "    # Extract features and target variable\n",
    "    X = average_cpi_by_year[['Model Year']]\n",
    "    y = average_cpi_by_year['CPI']\n",
    "\n",
    "    # Generate polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    # Fit the polynomial regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "\n",
    "    # Generate future 'Model Year' values\n",
    "    last_year = X['Model Year'].max()\n",
    "    future_years = np.arange(last_year + 1, last_year + 1 + forecast_periods).reshape(-1, 1)\n",
    "\n",
    "    # Transform future years to polynomial features\n",
    "    future_years_poly = poly.transform(future_years)\n",
    "\n",
    "    # Predict future CPI values\n",
    "    future_cpi = model.predict(future_years_poly)\n",
    "\n",
    "    # Create DataFrame for future predictions\n",
    "    future_df = pd.DataFrame({\n",
    "        'Model Year': future_years.flatten(),\n",
    "        'CPI': future_cpi\n",
    "    })\n",
    "\n",
    "    # Combine original and forecasted data\n",
    "    combined_df = pd.concat([average_cpi_by_year, future_df], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "average_cpi_by_year = forecast_cpi_polynomial(average_cpi_by_year, 2, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input_data(data):\n",
    "    # add the CPI of Used Cars by City Average\n",
    "    clean_data = data.merge(average_cpi_by_year, on='Model Year', how='left')\n",
    "    clean_data['Model Year'] = clean_data['Model Year'].astype(str)\n",
    "    \n",
    "    # handle nulls\n",
    "    clean_data = clean_data.replace({\"nan\": np.nan})\n",
    "    clean_data['Model Year'] = clean_data['Model Year'].fillna(\"Missing\")\n",
    "\n",
    "    # fill missing CPI's with average CPI\n",
    "    clean_data['CPI'] = clean_data['CPI'].fillna(clean_data['CPI'].mean())\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "class ConstantModel:\n",
    "    # A fallback model that always predicts a constant value\n",
    "    def __init__(self, value):\n",
    "        self.value = value  # Store the constant target value\n",
    "\n",
    "    def predict(self, X):\n",
    "        #  mimic contant\n",
    "        if not hasattr(X, '__len__'):  # Ensure X has a length (is iterable)\n",
    "            return self.value  # Just return the value if X isn't iterable\n",
    "        return [self.value] * len(X)  # Normal case\n",
    "\n",
    "\n",
    "def fit_catboost(X, y):\n",
    "    try:\n",
    "\n",
    "        categorical_cols = list(X.columns)\n",
    "        if 'CPI' in categorical_cols:\n",
    "            categorical_cols.remove('CPI')\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=100,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            cat_features=categorical_cols,\n",
    "            random_seed=42,\n",
    "            allow_writing_files=False,  # Prevents CatBoost from creating the catboost_info directory\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Clear CatBoost temporary files\n",
    "        catboost_tmp_dir = tempfile.gettempdir()  # Get system temp dir\n",
    "        for root, dirs, files in os.walk(catboost_tmp_dir):\n",
    "            for dir_name in dirs:\n",
    "                if \"catboost\" in dir_name:\n",
    "                    shutil.rmtree(os.path.join(root, dir_name), ignore_errors=True)\n",
    "\n",
    "        return model\n",
    "    except:\n",
    "        singular_value = y.iloc[0] if hasattr(y, 'iloc') else y[0]\n",
    "        return ConstantModel(singular_value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache_and_temp_files(root_dir='.'):\n",
    "    \"\"\"\n",
    "    Recursively deletes __pycache__ directories and .pyc files starting from the root_dir.\n",
    "    \"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # Remove __pycache__ directories\n",
    "        if '__pycache__' in dirnames:\n",
    "            pycache_path = os.path.join(dirpath, '__pycache__')\n",
    "            shutil.rmtree(pycache_path, ignore_errors=True)\n",
    "            print(f\"Deleted directory: {pycache_path}\")\n",
    "        \n",
    "        # Remove .pyc files\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.pyc'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted file: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_training = clean_input_data(training)\n",
    "clean_scoring = clean_input_data(scoring)\n",
    "\n",
    "car_data = pd.concat([clean_training, clean_scoring]).reset_index(drop=True)\n",
    "\n",
    "categorical_cols = car_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_car_data =  pd.get_dummies(car_data, columns=categorical_cols)\n",
    "\n",
    "train_indices = range(0, len(clean_training))\n",
    "test_indices = range(len(clean_training), len(car_data))\n",
    "\n",
    "train = car_data.loc[train_indices].copy()\n",
    "test = car_data.loc[test_indices].copy()\n",
    "\n",
    "train_encoded = encoded_car_data.loc[train_indices].copy()\n",
    "test_encoded = encoded_car_data.loc[test_indices].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_by_basis(car_data, encoded_car_data, train_indices, basis_columns, prediction_columns, target_col, model_func, encode):\n",
    "\n",
    "    train_data = car_data.loc[train_indices]\n",
    "    encoded_train = encoded_car_data.loc[train_indices]\n",
    "\n",
    "    # Define the columns used to create different models\n",
    "    basis_values = [train_data[col].unique().tolist() for col in basis_columns] # these are the potential values for each basis\n",
    "\n",
    "    # Generate all possible (Model Year, Fuel Type) combinations as tuples\n",
    "    basis_combinations = [tuple(values) for values in product(*basis_values)] # these are the combos of basis's so like 2020 Electric for example\n",
    "\n",
    "    # Dictionary to store trained models, using (Model Year, Fuel Type) as the key\n",
    "    trained_models = {}\n",
    "    for basis_key in basis_combinations:\n",
    "\n",
    "        basis_dict = {col: value for col, value in zip(basis_columns, basis_key)}\n",
    "        # Filter training data for the specific Model Year & Fuel Type\n",
    "        basis_indices = train_data[\n",
    "            train_data[basis_dict.keys()].eq(pd.Series(basis_dict)).all(axis=1)\n",
    "        ].index \n",
    "\n",
    "        # check if encoded\n",
    "        model_data = encoded_train.copy() if encode else train_data.copy()\n",
    "\n",
    "        # initialize X and y\n",
    "        X = model_data.loc[basis_indices, prediction_columns] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "        y = model_data.loc[basis_indices][target_col] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "\n",
    "\n",
    "        if len(X) > 10: # if there are at least 10 values\n",
    "            model = model_func(X, y)\n",
    "            safe = True\n",
    "        else:\n",
    "            model = model_func(model_data[prediction_columns], model_data[target_col])\n",
    "            safe = False\n",
    "\n",
    "        # Store trained model using the (Model Year, Fuel Type) tuple as the key\n",
    "        trained_models[basis_key] = (model, safe)\n",
    "\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, prediction_columns, target_col, encode):\n",
    "    all_prediction_rows = []\n",
    "    test = car_data.loc[test_indices]\n",
    "\n",
    "    for test_idx in test_indices:\n",
    "\n",
    "        row = car_data.loc[test_idx]\n",
    "\n",
    "        # get the basis values in the row (like 2020 for Model Year if Model Year is a basis)\n",
    "        model_basis_dict = {col: row[col] for col in basis_columns}\n",
    "\n",
    "        # get the model based on the basis\n",
    "        model, safe = trained_models[tuple(model_basis_dict.values())]\n",
    "\n",
    "        # get the encoded row\n",
    "        if encode:\n",
    "            formatted_row = encoded_car_data.loc[test_idx]\n",
    "            formatted_row = formatted_row[prediction_columns]\n",
    "        else:\n",
    "            formatted_row = row[prediction_columns]\n",
    "\n",
    "        # get the prediction vs. actual\n",
    "        prediction = model.predict(pd.DataFrame(formatted_row).T)[0]\n",
    "        actual = test.loc[test_idx, target_col]\n",
    "\n",
    "        prediction_row = pd.DataFrame({\n",
    "            'index': [test_idx],\n",
    "            'prediction': [round(prediction, 0)],\n",
    "            'actual': [actual],\n",
    "            'safe': [safe]\n",
    "        })\n",
    "        all_prediction_rows.append(prediction_row)\n",
    "\n",
    "    prediction_df = pd.concat(all_prediction_rows)\n",
    "    rmse = np.sqrt(mean_squared_error(prediction_df['actual'], prediction_df['prediction']))\n",
    "\n",
    "    return rmse, prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the combinations of basis columns and prediction columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_options = list(set(car_data.columns) - {'Date', 'Vehicle Population'})\n",
    "\n",
    "basis_prediction_combos = []\n",
    "for _ in range(250):\n",
    "    # Select 1-3 random basis columns\n",
    "    basis_columns = random.sample(column_options, random.randint(1, 2))\n",
    "    if 'CPI' in basis_columns:\n",
    "        basis_columns.remove('CPI')\n",
    "    if len(basis_columns) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Select 3-7 prediction columns that are **not in basis_columns**\n",
    "    remaining_columns = list(set(column_options) - set(basis_columns))\n",
    "    prediction_columns = random.sample(remaining_columns, random.randint(3, 8-len(basis_columns)))\n",
    "    \n",
    "    # Store as a tuple\n",
    "    basis_prediction_combos.append((basis_columns, prediction_columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 64, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load existing results if the file exists\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(results_file):\n\u001b[0;32m---> 10\u001b[0m     model_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(results_file)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     model_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasis_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 64, saw 5\n"
     ]
    }
   ],
   "source": [
    "# Define function to process each set of columns\n",
    "model_func = fit_catboost\n",
    "encode = False\n",
    "\n",
    "# Define file path for saving results\n",
    "results_file = '../../results/catboost_optim.csv'\n",
    "\n",
    "# Load existing results if the file exists\n",
    "if os.path.exists(results_file):\n",
    "    model_results = pd.read_csv(results_file)\n",
    "else:\n",
    "    model_results = pd.DataFrame(columns=['basis_columns', 'prediction_columns', 'rmse'])\n",
    "\n",
    "# Convert existing basis-prediction combinations to a set for quick lookup\n",
    "existing_combos = set(zip(model_results['basis_columns'].astype(str), model_results['prediction_columns'].astype(str)))\n",
    "\n",
    "target_col = 'Vehicle Population'\n",
    "\n",
    "def process_combo(basis_columns, prediction_columns, save=True):\n",
    "    try:\n",
    "        \n",
    "        # Convert combo to string format for checking existence\n",
    "        combo_key = (str(basis_columns), str(prediction_columns))\n",
    "        if combo_key in existing_combos:\n",
    "            print(f\"Skipping already processed: {basis_columns}, {prediction_columns}\")\n",
    "            return None\n",
    "\n",
    "        # Ensure prediction columns do not include basis columns\n",
    "        if encode:\n",
    "            clean_prediction_columns = list(set(col for col in train_encoded.columns if col.split(\"_\")[0] not in basis_columns) - {target_col, 'Date'})\n",
    "        else:\n",
    "            clean_prediction_columns = prediction_columns\n",
    "        \n",
    "        # Train models and calculate RMSE\n",
    "        trained_models = get_models_by_basis(car_data, encoded_car_data, train_indices, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n",
    "        rmse = calculate_test_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, clean_prediction_columns, target_col, encode)[0]\n",
    "\n",
    "        # Create DataFrame for this iteration\n",
    "        result_df = pd.DataFrame({\n",
    "            'basis_columns': [basis_columns],\n",
    "            'prediction_columns': [prediction_columns],\n",
    "            'rmse': [rmse]\n",
    "        })\n",
    "\n",
    "        if save:\n",
    "            # Append results to CSV immediately\n",
    "            result_df.to_csv(results_file, index=False, mode='a', header=not os.path.exists(results_file))\n",
    "\n",
    "        try:\n",
    "            shutil.rmtree('catboost_info')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "with tqdm_joblib(tqdm(desc=\"Processing Models\", total=len(basis_prediction_combos))) as progress_bar:\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_combo)(basis, pred) for basis, pred in basis_prediction_combos\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Models:   0%|          | 0/1 [00:53<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9160.065809717278"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_scoring =  encoded_car_data.loc[test_indices]\n",
    "\n",
    "# initialize inputs\n",
    "model_func = fit_catboost\n",
    "encode = False\n",
    "basis_columns = ['Vehicle Category', 'Fuel Type']\n",
    "prediction_columns = ['GVWR Class', 'CPI', 'Number of Vehicles Registered at the Same Address', 'Model Year', 'Fuel Technology']\n",
    "target_col = 'Vehicle Population'\n",
    "\n",
    "# clean prediction columns if necessary\n",
    "if encode:\n",
    "    clean_prediction_columns = list(set(col for col in encoded_scoring.columns if col.split(\"_\")[0] not in basis_columns) - {target_col, 'Date'})\n",
    "else:\n",
    "    clean_prediction_columns = prediction_columns\n",
    "\n",
    "trained_models = get_models_by_basis(car_data, encoded_car_data, train_indices, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n",
    "rmse, pred_df = calculate_test_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, clean_prediction_columns, target_col, encode)\n",
    "\n",
    "submission = pred_df[['prediction']].rename(columns={'prediction': 'Vehicle Population'}).reset_index(drop=True)\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
