{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: numpy==1.26.3 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 2)) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn==1.4.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: catboost==1.2.7 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 4)) (1.2.7)\n",
      "Requirement already satisfied: tqdm==4.66.2 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 5)) (4.66.2)\n",
      "Requirement already satisfied: joblib==1.3.2 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: tqdm_joblib==0.0.4 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 7)) (0.0.4)\n",
      "Requirement already satisfied: optuna==3.5.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: openpyxl==3.1.2 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 9)) (3.1.2)\n",
      "Requirement already satisfied: scikit-optimize==0.9.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 10)) (0.9.0)\n",
      "Requirement already satisfied: ipykernel==6.29.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from -r ../../requirements.txt (line 11)) (6.29.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from pandas==2.2.0->-r ../../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from pandas==2.2.0->-r ../../requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from pandas==2.2.0->-r ../../requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from scikit-learn==1.4.0->-r ../../requirements.txt (line 3)) (1.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from scikit-learn==1.4.0->-r ../../requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: graphviz in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from catboost==1.2.7->-r ../../requirements.txt (line 4)) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from catboost==1.2.7->-r ../../requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: plotly in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from catboost==1.2.7->-r ../../requirements.txt (line 4)) (6.0.0)\n",
      "Requirement already satisfied: six in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from catboost==1.2.7->-r ../../requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from optuna==3.5.0->-r ../../requirements.txt (line 8)) (1.14.1)\n",
      "Requirement already satisfied: colorlog in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from optuna==3.5.0->-r ../../requirements.txt (line 8)) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from optuna==3.5.0->-r ../../requirements.txt (line 8)) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from optuna==3.5.0->-r ../../requirements.txt (line 8)) (2.0.37)\n",
      "Requirement already satisfied: PyYAML in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from optuna==3.5.0->-r ../../requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: et-xmlfile in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from openpyxl==3.1.2->-r ../../requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from scikit-optimize==0.9.0->-r ../../requirements.txt (line 10)) (25.1.0)\n",
      "Requirement already satisfied: appnope in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (1.8.12)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (8.32.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (6.1.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (5.14.3)\n",
      "Requirement already satisfied: Mako in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna==3.5.0->-r ../../requirements.txt (line 8)) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna==3.5.0->-r ../../requirements.txt (line 8)) (4.12.2)\n",
      "Requirement already satisfied: decorator in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (4.3.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from matplotlib->catboost==1.2.7->-r ../../requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from matplotlib->catboost==1.2.7->-r ../../requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from matplotlib->catboost==1.2.7->-r ../../requirements.txt (line 4)) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from matplotlib->catboost==1.2.7->-r ../../requirements.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from matplotlib->catboost==1.2.7->-r ../../requirements.txt (line 4)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from matplotlib->catboost==1.2.7->-r ../../requirements.txt (line 4)) (3.2.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from plotly->catboost==1.2.7->-r ../../requirements.txt (line 4)) (1.24.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna==3.5.0->-r ../../requirements.txt (line 8)) (3.0.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.29.0->-r ../../requirements.txt (line 11)) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kd/nlngvn_j3cddf9vyvh_kn5fh0000gn/T/ipykernel_8957/1964758745.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages/tqdm_joblib/__init__.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import optuna\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willmiraglia/Documents/School/RICE/Datathon 2025/.venv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PolynomialFeatures was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Download Data Dictionary\n",
    "data_dict_path = '../../data/data_dictionary.xlsx'\n",
    "data_dictionary = pd.read_excel(data_dict_path)\n",
    "\n",
    "# Download Scoring File -- what the input data will be for the end\n",
    "scoring_path = '../../data/scoring.xlsx'\n",
    "scoring = pd.read_excel(scoring_path)\n",
    "\n",
    "# Download Submission Format -- what the format of the output should be from my model\n",
    "submission_format_path = '../../data/submission_format.csv'\n",
    "submission_format = pd.read_csv(submission_format_path)\n",
    "\n",
    "# Download Training Data -- what the training data is for this\n",
    "training_path = '../../data/training.xlsx'\n",
    "training = pd.read_excel(training_path).dropna()\n",
    "\n",
    "# new data from Fred: https://fred.stlouisfed.org/series/CUSR0000SETA02\n",
    "cpi_used_cars_path = '../../data/CPI_UsedCars_US.xlsx'\n",
    "cpi_used_cars = pd.read_excel(cpi_used_cars_path, sheet_name='Monthly')\n",
    "cpi_used_cars.columns = ['Date', 'CPI']\n",
    "cpi_used_cars['Year'] = cpi_used_cars['Date'].dt.year\n",
    "average_cpi_by_year = cpi_used_cars.groupby('Year')['CPI'].mean().reset_index().rename(columns={'Year': 'Model Year'})\n",
    "# since average cpi by year is missing 2025 and 2026, forecast using a neural prophet model\n",
    "def forecast_cpi_polynomial(average_cpi_by_year, forecast_periods=5, degree=2):\n",
    "    # Ensure data is sorted by 'Model Year'\n",
    "    average_cpi_by_year = average_cpi_by_year.sort_values(by='Model Year')\n",
    "\n",
    "    # Extract features and target variable\n",
    "    X = average_cpi_by_year[['Model Year']]\n",
    "    y = average_cpi_by_year['CPI']\n",
    "\n",
    "    # Generate polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    # Fit the polynomial regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "\n",
    "    # Generate future 'Model Year' values\n",
    "    last_year = X['Model Year'].max()\n",
    "    future_years = np.arange(last_year + 1, last_year + 1 + forecast_periods).reshape(-1, 1)\n",
    "\n",
    "    # Transform future years to polynomial features\n",
    "    future_years_poly = poly.transform(future_years)\n",
    "\n",
    "    # Predict future CPI values\n",
    "    future_cpi = model.predict(future_years_poly)\n",
    "\n",
    "    # Create DataFrame for future predictions\n",
    "    future_df = pd.DataFrame({\n",
    "        'Model Year': future_years.flatten(),\n",
    "        'CPI': future_cpi\n",
    "    })\n",
    "\n",
    "    # Combine original and forecasted data\n",
    "    combined_df = pd.concat([average_cpi_by_year, future_df], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "average_cpi_by_year = forecast_cpi_polynomial(average_cpi_by_year, 2, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input_data(data):\n",
    "    # add the CPI of Used Cars by City Average\n",
    "    clean_data = data.merge(average_cpi_by_year, on='Model Year', how='left')\n",
    "    clean_data['Model Year'] = clean_data['Model Year'].astype(str)\n",
    "    \n",
    "    # handle nulls\n",
    "    clean_data = clean_data.replace({\"nan\": np.nan})\n",
    "    clean_data['Model Year'] = clean_data['Model Year'].fillna(\"Missing\")\n",
    "\n",
    "    # fill missing CPI's with average CPI\n",
    "    clean_data['CPI'] = clean_data['CPI'].fillna(clean_data['CPI'].mean())\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "class ConstantModel:\n",
    "    # A fallback model that always predicts a constant value\n",
    "    def __init__(self, value):\n",
    "        self.value = value  # Store the constant target value\n",
    "\n",
    "    def predict(self, X):\n",
    "        #  mimic contant\n",
    "        if not hasattr(X, '__len__'):  # Ensure X has a length (is iterable)\n",
    "            return self.value  # Just return the value if X isn't iterable\n",
    "        return [self.value] * len(X)  # Normal case\n",
    "\n",
    "\n",
    "def fit_catboost(X, y):\n",
    "    try:\n",
    "\n",
    "        categorical_cols = list(X.columns)\n",
    "        if 'CPI' in categorical_cols:\n",
    "            categorical_cols.remove('CPI')\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=100,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            cat_features=categorical_cols,\n",
    "            random_seed=42,\n",
    "            allow_writing_files=False,  # Prevents CatBoost from creating the catboost_info directory\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Clear CatBoost temporary files\n",
    "        catboost_tmp_dir = tempfile.gettempdir()  # Get system temp dir\n",
    "        for root, dirs, files in os.walk(catboost_tmp_dir):\n",
    "            for dir_name in dirs:\n",
    "                if \"catboost\" in dir_name:\n",
    "                    shutil.rmtree(os.path.join(root, dir_name), ignore_errors=True)\n",
    "\n",
    "        return model\n",
    "    except:\n",
    "        singular_value = y.iloc[0] if hasattr(y, 'iloc') else y[0]\n",
    "        return ConstantModel(singular_value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_training = clean_input_data(training)\n",
    "clean_scoring = clean_input_data(scoring)\n",
    "\n",
    "car_data = pd.concat([clean_training, clean_scoring]).reset_index(drop=True)\n",
    "car_data['Model Year'] = car_data['Model Year'].astype(str)\n",
    "\n",
    "categorical_cols = car_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_car_data =  pd.get_dummies(car_data, columns=categorical_cols)\n",
    "\n",
    "train_indices = range(0, len(clean_training))\n",
    "test_indices = range(len(clean_training), len(car_data))\n",
    "\n",
    "train = car_data.loc[train_indices].copy()\n",
    "test = car_data.loc[test_indices].copy()\n",
    "\n",
    "train_encoded = encoded_car_data.loc[train_indices].copy()\n",
    "test_encoded = encoded_car_data.loc[test_indices].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_by_basis(car_data, encoded_car_data, train_indices, basis_columns, prediction_columns, target_col, model_func, encode):\n",
    "\n",
    "    train_data = car_data.loc[train_indices]\n",
    "    encoded_train = encoded_car_data.loc[train_indices]\n",
    "\n",
    "    # Define the columns used to create different models\n",
    "    basis_values = [train_data[col].unique().tolist() for col in basis_columns] # these are the potential values for each basis\n",
    "\n",
    "    # Generate all possible (Model Year, Fuel Type) combinations as tuples\n",
    "    basis_combinations = [tuple(values) for values in product(*basis_values)] # these are the combos of basis's so like 2020 Electric for example\n",
    "\n",
    "    # Dictionary to store trained models, using (Model Year, Fuel Type) as the key\n",
    "    trained_models = {}\n",
    "    for basis_key in basis_combinations:\n",
    "\n",
    "        basis_dict = {col: value for col, value in zip(basis_columns, basis_key)}\n",
    "        # Filter training data for the specific Model Year & Fuel Type\n",
    "        basis_indices = train_data[\n",
    "            train_data[basis_dict.keys()].eq(pd.Series(basis_dict)).all(axis=1)\n",
    "        ].index \n",
    "\n",
    "        # check if encoded\n",
    "        model_data = encoded_train.copy() if encode else train_data.copy()\n",
    "\n",
    "        # initialize X and y\n",
    "        X = model_data.loc[basis_indices, prediction_columns] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "        y = model_data.loc[basis_indices][target_col] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "\n",
    "\n",
    "        if len(X) > 10: # if there are at least 10 values\n",
    "            model = model_func(X, y)\n",
    "            safe = True\n",
    "        else:\n",
    "            model = model_func(model_data[prediction_columns], model_data[target_col])\n",
    "            safe = False\n",
    "\n",
    "        # Store trained model using the (Model Year, Fuel Type) tuple as the key\n",
    "        trained_models[basis_key] = (model, safe)\n",
    "\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_by_basis_with_hyperparams(car_data, encoded_car_data, basis_columns, prediction_columns, target_col, model_func, encode, params_dict):\n",
    "    # getting the models with specified hyperparameters. Make sure params dict keys are the same as the basis combinations ***\n",
    "\n",
    "    train_data = car_data.loc[train_indices]\n",
    "    encoded_train = encoded_car_data.loc[train_indices]\n",
    "\n",
    "    # Define the columns used to create different models\n",
    "    basis_values = [train_data[col].unique().tolist() for col in basis_columns] # these are the potential values for each basis\n",
    "\n",
    "    # Generate all possible (Model Year, Fuel Type) combinations as tuples\n",
    "    basis_combinations = [tuple(values) for values in product(*basis_values)] # these are the combos of basis's so like 2020 Electric for example\n",
    "\n",
    "    # Dictionary to store trained models, using (Model Year, Fuel Type) as the key\n",
    "    trained_models = {}\n",
    "    for basis_key in basis_combinations:\n",
    "\n",
    "        optimal_params = params_dict[basis_key]\n",
    "\n",
    "        basis_dict = {col: value for col, value in zip(basis_columns, basis_key)}\n",
    "        # Filter training data for the specific Model Year & Fuel Type\n",
    "        basis_indices = train_data[\n",
    "            train_data[basis_dict.keys()].eq(pd.Series(basis_dict)).all(axis=1)\n",
    "        ].index \n",
    "\n",
    "        # check if encoded\n",
    "        model_data = encoded_train.copy() if encode else train_data.copy()\n",
    "        # initialize X and y\n",
    "        X = model_data.loc[basis_indices, prediction_columns] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "        y = model_data.loc[basis_indices][target_col] if len(basis_indices) != 0 else pd.DataFrame()\n",
    "\n",
    "        cat_features = [col for col in prediction_columns if train_data[col].dtype == 'object']\n",
    "        model = CatBoostRegressor(**optimal_params, cat_features=cat_features)\n",
    "        if len(X) > 10: # if there are at least 10 values\n",
    "            model.fit(X, y)\n",
    "            safe = True\n",
    "        else:\n",
    "            model.fit(model_data[prediction_columns], model_data[target_col])\n",
    "            safe = False\n",
    "\n",
    "        # Store trained model using the (Model Year, Fuel Type) tuple as the key\n",
    "        trained_models[basis_key] = (model, safe)\n",
    "\n",
    "    return trained_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, prediction_columns, target_col, encode):\n",
    "    all_prediction_rows = []\n",
    "    test = car_data.loc[test_indices]\n",
    "\n",
    "    for test_idx in test_indices:\n",
    "\n",
    "        row = car_data.loc[test_idx]\n",
    "\n",
    "        # get the basis values in the row (like 2020 for Model Year if Model Year is a basis)\n",
    "        model_basis_dict = {col: row[col] for col in basis_columns}\n",
    "\n",
    "        # get the model based on the basis\n",
    "        model, safe = trained_models[tuple(model_basis_dict.values())]\n",
    "\n",
    "        # get the encoded row\n",
    "        if encode:\n",
    "            formatted_row = encoded_car_data.loc[test_idx]\n",
    "            formatted_row = formatted_row[prediction_columns]\n",
    "        else:\n",
    "            formatted_row = row[prediction_columns]\n",
    "\n",
    "        # get the prediction vs. actual\n",
    "        prediction = model.predict(pd.DataFrame(formatted_row).T)[0]\n",
    "        actual = test.loc[test_idx, target_col]\n",
    "\n",
    "        prediction_row = pd.DataFrame({\n",
    "            'index': [test_idx],\n",
    "            'prediction': [round(prediction, 0)],\n",
    "            'actual': [actual],\n",
    "            'safe': [safe]\n",
    "        })\n",
    "        all_prediction_rows.append(prediction_row)\n",
    "\n",
    "    prediction_df = pd.concat(all_prediction_rows)\n",
    "    rmse = np.sqrt(mean_squared_error(prediction_df['actual'], prediction_df['prediction']))\n",
    "\n",
    "    return rmse, prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i>Get the combinations of basis columns and prediction columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_options = list(set(car_data.columns) - {'Date', 'Vehicle Population'})\n",
    "\n",
    "basis_prediction_combos = []\n",
    "for _ in range(250):\n",
    "    # Select 1-3 random basis columns\n",
    "    basis_columns = random.sample(column_options, random.randint(1, 2))\n",
    "    if 'CPI' in basis_columns:\n",
    "        basis_columns.remove('CPI')\n",
    "    if len(basis_columns) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Select 3-7 prediction columns that are **not in basis_columns**\n",
    "    remaining_columns = list(set(column_options) - set(basis_columns))\n",
    "    prediction_columns = random.sample(remaining_columns, random.randint(3, 8-len(basis_columns)))\n",
    "    \n",
    "    # Store as a tuple\n",
    "    basis_prediction_combos.append((basis_columns, prediction_columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i>Run tests to find best basis and prediction columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "runOptimization = False\n",
    "\n",
    "# Define function to process each set of columns\n",
    "model_func = fit_linear_regression\n",
    "encode = True\n",
    "\n",
    "# Define file path for saving results\n",
    "results_file = f'../../results/{model_func.__name__}_optim.csv'\n",
    "\n",
    "# Load existing results if the file exists\n",
    "if os.path.exists(results_file):\n",
    "    model_results = pd.read_csv(results_file)\n",
    "else:\n",
    "    model_results = pd.DataFrame(columns=['basis_columns', 'prediction_columns', 'rmse'])\n",
    "\n",
    "# Convert existing basis-prediction combinations to a set for quick lookup\n",
    "existing_combos = set(zip(model_results['basis_columns'].astype(str), model_results['prediction_columns'].astype(str)))\n",
    "\n",
    "target_col = 'Vehicle Population'\n",
    "\n",
    "def process_combo(basis_columns, prediction_columns, save=True):\n",
    "    try:\n",
    "        \n",
    "        # Convert combo to string format for checking existence\n",
    "        combo_key = (str(basis_columns), str(prediction_columns))\n",
    "        if combo_key in existing_combos:\n",
    "            print(f\"Skipping already processed: {basis_columns}, {prediction_columns}\")\n",
    "            return None\n",
    "\n",
    "        # Ensure prediction columns do not include basis columns\n",
    "        if encode:\n",
    "            clean_prediction_columns = list(set(col for col in train_encoded.columns if col.split(\"_\")[0] not in basis_columns) - {target_col, 'Date'})\n",
    "        else:\n",
    "            clean_prediction_columns = prediction_columns\n",
    "        \n",
    "        # Train models and calculate RMSE\n",
    "        trained_models = get_models_by_basis(car_data, encoded_car_data, train_indices, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n",
    "        rmse = calculate_test_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, clean_prediction_columns, target_col, encode)[0]\n",
    "\n",
    "        # Create DataFrame for this iteration\n",
    "        result_df = pd.DataFrame({\n",
    "            'basis_columns': [basis_columns],\n",
    "            'prediction_columns': [prediction_columns],\n",
    "            'rmse': [rmse]\n",
    "        })\n",
    "\n",
    "        if save:\n",
    "            # Append results to CSV immediately\n",
    "            result_df.to_csv(results_file, index=False, mode='a', header=not os.path.exists(results_file))\n",
    "\n",
    "        try:\n",
    "            shutil.rmtree('catboost_info')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "if runOptimization:\n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm_joblib(tqdm(desc=\"Processing Models\", total=len(basis_prediction_combos))) as progress_bar:\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(process_combo)(basis, pred) for basis, pred in basis_prediction_combos\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i>Bayesian Optimziation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y, cat_features):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),                    \n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),     \n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),                  \n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 1),            \n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),  \n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),           \n",
    "        'verbose': 0\n",
    "    }\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    model = CatBoostRegressor(**params, cat_features=cat_features)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "    preds = model.predict(X_val)\n",
    "    return np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "def optimize_catboost(X, y, cat_features, n_trials=20):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, X, y, cat_features), n_trials=n_trials, timeout=30)\n",
    "    best_params = study.best_params\n",
    "\n",
    "    model = CatBoostRegressor(**best_params, cat_features=cat_features)\n",
    "    model.fit(X, y, verbose=0)\n",
    "    return model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_by_basis_optimization(train_data, train_encoded, basis_columns, prediction_columns, target_col, model_func, encode):\n",
    "\n",
    "    basis_values = [train_data[col].unique().tolist() for col in basis_columns]\n",
    "    basis_combinations = [tuple(values) for values in product(*basis_values)]\n",
    "\n",
    "    cat_features = [col for col in prediction_columns if train_data[col].dtype == 'object']\n",
    "\n",
    "    if not cat_features:\n",
    "        cat_features = []\n",
    "\n",
    "    trained_models = {}\n",
    "    for basis_key in basis_combinations:\n",
    "        basis_dict = {col: value for col, value in zip(basis_columns, basis_key)}\n",
    "        \n",
    "        basis_indices = train_data[\n",
    "            train_data[list(basis_dict.keys())].eq(pd.Series(basis_dict)).all(axis=1)\n",
    "        ].index\n",
    "\n",
    "        model_data = train_encoded.copy() if encode else train_data.copy()\n",
    "        X = model_data.loc[basis_indices, prediction_columns]\n",
    "        y = model_data.loc[basis_indices, target_col]\n",
    "\n",
    "        if len(X) > 10:  # Sufficient data\n",
    "            model, params = optimize_catboost(X, y, cat_features)\n",
    "            safe = True\n",
    "        else:  # Fallback model for small datasets\n",
    "            model = model_func(model_data[prediction_columns], model_data[target_col])\n",
    "            params = None\n",
    "            safe = False\n",
    "\n",
    "        trained_models[basis_key] = (model, safe, params)\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "\n",
    "def calculate_rmse_optimization(test_data, encoded_test_data, trained_models, basis_columns, prediction_columns, target_col, encode):\n",
    "    all_predictions = []\n",
    "\n",
    "    for idx, row in test_data.iterrows():\n",
    "        model_basis_key = tuple(row[basis_columns])\n",
    "        model_info = trained_models.get(model_basis_key, (None, False, None))\n",
    "        model, safe, params = model_info\n",
    "\n",
    "        if model:\n",
    "            formatted_row = (encoded_test_data if encode else test_data).loc[idx, prediction_columns]\n",
    "            prediction = model.predict(pd.DataFrame(formatted_row).T)[0]\n",
    "            actual = row[target_col]\n",
    "\n",
    "            all_predictions.append({\n",
    "                'index': idx,\n",
    "                'prediction': round(prediction, 0),\n",
    "                'actual': actual,\n",
    "                'basis_key': model_basis_key,\n",
    "                'safe': safe\n",
    "            })\n",
    "\n",
    "    if all_predictions:\n",
    "        pred_df = pd.DataFrame(all_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(pred_df['actual'], pred_df['prediction']))\n",
    "    else:\n",
    "        rmse = None  # No predictions made\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'results/catboost_bayesian_optimization.xlsx'\n",
    "\n",
    "def append_to_excel(df, file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        df.to_excel(file_path, index=False)\n",
    "    else:\n",
    "        with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "            workbook = load_workbook(file_path)\n",
    "            sheet = workbook.active\n",
    "            startrow = sheet.max_row\n",
    "            \n",
    "            df.to_excel(writer, index=False, header=False, startrow=startrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "runOptimization = False\n",
    "\n",
    "model_func = fit_catboost\n",
    "column_options = list(set(train.columns) - {'Date', 'Vehicle Population'})\n",
    "encode = False\n",
    "\n",
    "def process_basis_prediction_combo(basis_columns, prediction_columns):\n",
    "    try:\n",
    "        target_col = 'Vehicle Population'\n",
    "        \n",
    "        clean_prediction_columns = (\n",
    "            [col for col in train_encoded.columns if col.split(\"_\")[0] not in basis_columns and col not in [target_col, 'Date']]\n",
    "            if encode else prediction_columns\n",
    "        )\n",
    "        \n",
    "        trained_models = get_models_by_basis_optimization(train, train_encoded, basis_columns, clean_prediction_columns, target_col, model_func, encode)\n",
    "        \n",
    "        rmse = calculate_rmse_optimization(test, test_encoded, trained_models, basis_columns, clean_prediction_columns, target_col, encode)\n",
    "\n",
    "        basis_keys_params = {\n",
    "            basis_key: params for basis_key, (_, _, params) in trained_models.items()\n",
    "        }\n",
    "\n",
    "        results = pd.DataFrame({\n",
    "            'basis_columns': [basis_columns],\n",
    "            'basis_keys_used': [list(trained_models.keys())],\n",
    "            'basis_keys_params': [basis_keys_params],\n",
    "            'prediction_columns': [prediction_columns],\n",
    "            'rmse': [rmse]\n",
    "        })\n",
    "\n",
    "        print(results) \n",
    "\n",
    "        append_to_excel(results, output_path)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing combo {basis_columns}, {prediction_columns}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "model_results_list = []\n",
    "if runOptimization:\n",
    "    with tqdm(desc=\"Processing Models\", total=len(basis_prediction_combos)) as pbar:\n",
    "        model_results_list = Parallel(n_jobs=-1)(\n",
    "            delayed(process_basis_prediction_combo)(basis, pred) for basis, pred in basis_prediction_combos\n",
    "        )\n",
    "        pbar.update(len(basis_prediction_combos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7374.301964135726"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_scoring =  encoded_car_data.loc[test_indices]\n",
    "\n",
    "# initialize inputs\n",
    "model_func = fit_catboost\n",
    "encode = False\n",
    "basis_columns = ['Vehicle Category']\n",
    "prediction_columns = ['Fuel Technology', 'CPI', 'Number of Vehicles Registered at the Same Address', 'GVWR Class', 'Electric Mile Range', 'Fuel Type', 'Model Year']\n",
    "target_col = 'Vehicle Population'\n",
    "\n",
    "# clean prediction columns if necessary\n",
    "if encode:\n",
    "    clean_prediction_columns = list(set(col for col in encoded_scoring.columns if col.split(\"_\")[0] not in basis_columns) - {target_col, 'Date'})\n",
    "else:\n",
    "    clean_prediction_columns = prediction_columns\n",
    "\n",
    "basis_keys = [('P',), ('T2',), ('T3',), ('T1',), ('MC',), ('T4',), ('T7',), ('T6',), ('T5',), ('MH',), ('BS',), ('B',), ('BT',)]\n",
    "\n",
    "params_dict = {\n",
    "    ('P',): {'iterations': 853, 'learning_rate': 0.2429781556456842, 'depth': 6, 'l2_leaf_reg': 2.282136581605469, 'bagging_temperature': 0.21448680294505562, 'border_count': 202, 'random_strength': 0.5230550908979421, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 8, 'verbose':0},\n",
    "    ('T2',): {'iterations': 335, 'learning_rate': 0.26931213330418335, 'depth': 10, 'l2_leaf_reg': 9.640870180295874, 'bagging_temperature': 0.30115087324036427, 'border_count': 178, 'random_strength': 0.39990182837262866, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 14, 'verbose':0},\n",
    "    ('T3',): {'iterations': 865, 'learning_rate': 0.1010367015771094, 'depth': 9, 'l2_leaf_reg': 7.42764752331621, 'bagging_temperature': 0.6459945532391831, 'border_count': 136, 'random_strength': 0.3485326296696091, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 19, 'verbose':0},\n",
    "    ('T1',): {'iterations': 171, 'learning_rate': 0.10839852850884317, 'depth': 7, 'l2_leaf_reg': 6.465152613240907, 'bagging_temperature': 0.05956399621682334, 'border_count': 32, 'random_strength': 0.9969824754272121, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 1, 'verbose':0},\n",
    "    ('MC',): {'iterations': 493, 'learning_rate': 0.017972336996238937, 'depth': 5, 'l2_leaf_reg': 9.904395136404345, 'bagging_temperature': 0.003721695868538509, 'border_count': 119, 'random_strength': 0.9731604471489881, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 15, 'verbose':0},\n",
    "    ('T4',): {'iterations': 293, 'learning_rate': 0.16845525328753766, 'depth': 9, 'l2_leaf_reg': 7.3028486840112725, 'bagging_temperature': 0.1990811649266926, 'border_count': 84, 'random_strength': 0.8609406400707353, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 6, 'verbose':0},\n",
    "    ('T7',): {'iterations': 801, 'learning_rate': 0.08299420642146778, 'depth': 9, 'l2_leaf_reg': 1.9216760380872735, 'bagging_temperature': 0.0009397731362831428, 'border_count': 213, 'random_strength': 0.19396619269250526, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 16, 'verbose':0},\n",
    "    ('T6',): {'iterations': 610, 'learning_rate': 0.12293705953473834, 'depth': 8, 'l2_leaf_reg': 4.96309503860056, 'bagging_temperature': 0.873646019286267, 'border_count': 191, 'random_strength': 0.8196419736475136, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 9, 'verbose':0},\n",
    "    ('T5',): {'iterations': 563, 'learning_rate': 0.21256793340510682, 'depth': 7, 'l2_leaf_reg': 6.892637478520299, 'bagging_temperature': 0.9724910160820344, 'border_count': 191, 'random_strength': 0.4347428589259234, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 9, 'verbose':0},\n",
    "    ('MH',): {'iterations': 885, 'learning_rate': 0.26170267256322144, 'depth': 8, 'l2_leaf_reg': 6.994752208188576, 'bagging_temperature': 0.9067775235329057, 'border_count': 149, 'random_strength': 0.29522679258872647, 'grow_policy': 'Depthwise', 'min_data_in_leaf': 13, 'verbose': 0},\n",
    "    ('BS',): {'iterations': 765, 'learning_rate': 0.0863404017910146, 'depth': 4, 'l2_leaf_reg': 6.859286521399906, 'bagging_temperature': 0.7652808535846216, 'border_count': 141, 'random_strength': 0.6183738585814146, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 6, 'verbose': 0},\n",
    "    ('B',): {'iterations': 272, 'learning_rate': 0.23433232647434127, 'depth': 5, 'l2_leaf_reg': 3.0329940707082588, 'bagging_temperature': 0.13616676011461504, 'border_count': 238, 'random_strength': 0.7781224191172322, 'grow_policy': 'SymmetricTree', 'min_data_in_leaf': 18, 'verbose': 0},\n",
    "    ('BT',): {'iterations': 768, 'learning_rate': 0.2575430567722977, 'depth': 4, 'l2_leaf_reg': 1.6541315061037025, 'bagging_temperature': 0.628596056037448, 'border_count': 153, 'random_strength': 0.6865879212812991, 'grow_policy': 'Lossguide', 'min_data_in_leaf': 6, 'verbose': 0}\n",
    "}\n",
    "\n",
    "trained_models = get_models_by_basis_with_hyperparams(car_data, encoded_car_data, basis_columns, clean_prediction_columns, target_col, model_func, encode, params_dict)\n",
    "rmse, pred_df = calculate_test_rmse(car_data, encoded_car_data, test_indices, trained_models, basis_columns, clean_prediction_columns, target_col, encode)\n",
    "\n",
    "submission = pred_df[['prediction']].rename(columns={'prediction': 'Vehicle Population'}).reset_index(drop=True)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicle Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>287075.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>297439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>233580.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7541</th>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7542</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7543</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7544</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7545</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7546 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Vehicle Population\n",
       "0               287075.0\n",
       "1               266228.0\n",
       "2               297439.0\n",
       "3                94319.0\n",
       "4               233580.0\n",
       "...                  ...\n",
       "7541                 6.0\n",
       "7542                 2.0\n",
       "7543                 2.0\n",
       "7544                 5.0\n",
       "7545                 4.0\n",
       "\n",
       "[7546 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
